19:37:27,279 graphrag.cli.index INFO Logging enabled at C:\Users\ASUS\Downloads\Bachelor_research_project\backend\src\graphrag\graphrag_db\logs\indexing-engine.log
19:37:29,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:37:30,799 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
19:37:30,804 graphrag.cli.index INFO Starting pipeline run. dry_run=False
19:37:30,808 graphrag.cli.index INFO Using default configuration: {
    "root_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "encoding_model": "o200k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\logs",
        "storage_account_blob_url": null
    },
    "output": {
        "type": "file",
        "base_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "embed_text": {
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "names": [],
        "strategy": null,
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store"
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false
    },
    "extract_graph": {
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": null,
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "summarize_descriptions": {
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "community_reports": {
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "extract_claims": {
        "enabled": false,
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40,
        "remove_ego_nodes": false,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "reduce_max_tokens": 2000,
        "reduce_temperature": 0,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 4096
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "conversation_history_max_turns": 5,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null
}
19:37:30,811 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\ASUS\Downloads\Bachelor_research_project\backend\src\graphrag\graphrag_db\output
19:37:30,811 graphrag.index.input.factory INFO loading input from root_dir=input
19:37:30,811 graphrag.index.input.factory INFO using file storage for input
19:37:30,813 graphrag.storage.file_pipeline_storage INFO search C:\Users\ASUS\Downloads\Bachelor_research_project\backend\src\graphrag\graphrag_db\input for files matching .*\.txt$
19:39:02,976 graphrag.cli.index INFO Logging enabled at C:\Users\ASUS\Downloads\Bachelor_research_project\backend\src\graphrag\graphrag_db\logs\indexing-engine.log
19:39:03,986 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:05,170 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
19:39:05,179 graphrag.cli.index INFO Starting pipeline run. dry_run=False
19:39:05,179 graphrag.cli.index INFO Using default configuration: {
    "root_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "encoding_model": "o200k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\logs",
        "storage_account_blob_url": null
    },
    "output": {
        "type": "file",
        "base_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "embed_text": {
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "names": [],
        "strategy": null,
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store"
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false
    },
    "extract_graph": {
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": null,
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "summarize_descriptions": {
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "community_reports": {
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "extract_claims": {
        "enabled": false,
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40,
        "remove_ego_nodes": false,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "reduce_max_tokens": 2000,
        "reduce_temperature": 0,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 4096
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "conversation_history_max_turns": 5,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null
}
19:39:05,195 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\ASUS\Downloads\Bachelor_research_project\backend\src\graphrag\graphrag_db\output
19:39:05,195 graphrag.index.input.factory INFO loading input from root_dir=input
19:39:05,196 graphrag.index.input.factory INFO using file storage for input
19:39:05,198 graphrag.storage.file_pipeline_storage INFO search C:\Users\ASUS\Downloads\Bachelor_research_project\backend\src\graphrag\graphrag_db\input for files matching .*\.txt$
19:39:05,208 graphrag.index.input.util INFO Found 1 InputFileType.text files, loading 1
19:39:05,209 graphrag.index.input.util INFO Total number of unfiltered InputFileType.text rows: 1
19:39:05,212 graphrag.index.run.run_pipeline INFO Final # of rows loaded: 1
19:39:05,256 graphrag.utils.storage INFO reading table from storage: documents.parquet
19:39:05,349 graphrag.utils.storage INFO reading table from storage: documents.parquet
19:39:05,354 graphrag.utils.storage INFO reading table from storage: text_units.parquet
19:39:05,405 graphrag.utils.storage INFO reading table from storage: text_units.parquet
19:39:11,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:12,998 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:14,162 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:16,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:23,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:23,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:25,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:25,625 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:25,904 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:26,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:28,360 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:29,148 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:29,209 graphrag.utils.storage INFO reading table from storage: entities.parquet
19:39:29,228 graphrag.utils.storage INFO reading table from storage: relationships.parquet
19:39:29,302 graphrag.utils.storage INFO reading table from storage: entities.parquet
19:39:29,326 graphrag.utils.storage INFO reading table from storage: relationships.parquet
19:39:29,414 graphrag.utils.storage INFO reading table from storage: text_units.parquet
19:39:29,418 graphrag.utils.storage INFO reading table from storage: entities.parquet
19:39:29,421 graphrag.utils.storage INFO reading table from storage: relationships.parquet
19:39:29,475 graphrag.utils.storage INFO reading table from storage: relationships.parquet
19:39:29,481 graphrag.utils.storage INFO reading table from storage: entities.parquet
19:39:29,486 graphrag.utils.storage INFO reading table from storage: communities.parquet
19:39:29,528 graphrag.index.operations.summarize_communities.graph_context.context_builder INFO Number of nodes at level=0 => 16
19:39:42,87 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:43,300 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:43,437 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
19:39:43,506 graphrag.utils.storage INFO reading table from storage: documents.parquet
19:39:43,527 graphrag.utils.storage INFO reading table from storage: relationships.parquet
19:39:43,531 graphrag.utils.storage INFO reading table from storage: text_units.parquet
19:39:43,550 graphrag.utils.storage INFO reading table from storage: entities.parquet
19:39:43,553 graphrag.utils.storage INFO reading table from storage: community_reports.parquet
19:39:43,582 graphrag.index.workflows.generate_text_embeddings INFO Creating embeddings
19:39:43,582 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
19:39:43,622 graphrag.index.operations.embed_text.strategies.openai INFO embedding 3 inputs via 3 snippets using 1 batches. max_batch_size=16, max_tokens=8191
19:39:44,923 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
19:39:45,480 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
19:39:45,483 graphrag.index.operations.embed_text.strategies.openai INFO embedding 18 inputs via 18 snippets using 2 batches. max_batch_size=16, max_tokens=8191
19:39:46,92 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
19:39:46,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
19:39:46,851 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
19:39:46,855 graphrag.index.operations.embed_text.strategies.openai INFO embedding 3 inputs via 3 snippets using 1 batches. max_batch_size=16, max_tokens=8191
19:39:49,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
19:39:49,622 graphrag.cli.index INFO All workflows completed successfully.
13:37:31,580 graphrag.cli.index INFO Logging enabled at /app/src/graphrag/graphrag_db/logs/indexing-engine.log
13:37:32,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:37:34,456 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:37:34,460 graphrag.cli.index INFO Starting pipeline run. dry_run=False
13:37:34,461 graphrag.cli.index INFO Using default configuration: {
    "root_dir": "/app/src/graphrag/graphrag_db",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "encoding_model": "o200k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "/app/src/graphrag/graphrag_db/logs",
        "storage_account_blob_url": null
    },
    "output": {
        "type": "file",
        "base_dir": "/app/src/graphrag/graphrag_db/output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "/app/src/graphrag/graphrag_db/update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "embed_text": {
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "names": [],
        "strategy": null,
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store"
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false
    },
    "extract_graph": {
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": null,
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "summarize_descriptions": {
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "community_reports": {
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "extract_claims": {
        "enabled": false,
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40,
        "remove_ego_nodes": false,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "reduce_max_tokens": 2000,
        "reduce_temperature": 0,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 4096
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "conversation_history_max_turns": 5,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "/app/src/graphrag/graphrag_db/output/lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null
}
13:37:34,464 graphrag.storage.file_pipeline_storage INFO Creating file storage at /app/src/graphrag/graphrag_db/output
13:37:34,470 graphrag.index.input.factory INFO loading input from root_dir=input
13:37:34,471 graphrag.index.input.factory INFO using file storage for input
13:37:34,474 graphrag.storage.file_pipeline_storage INFO search /app/src/graphrag/graphrag_db/input for files matching .*\.txt$
13:37:34,511 graphrag.index.input.util INFO Found 2 InputFileType.text files, loading 2
13:37:34,513 graphrag.index.input.util INFO Total number of unfiltered text rows: 2
13:37:34,515 graphrag.utils.storage INFO reading table from storage: documents.parquet
13:37:34,590 graphrag.storage.file_pipeline_storage INFO Creating file storage at /app/src/graphrag/graphrag_db/update_output
13:37:34,600 graphrag.storage.file_pipeline_storage INFO search /app/src/graphrag/graphrag_db/output for files matching \.parquet$
13:37:34,987 graphrag.utils.storage INFO reading table from storage: communities.parquet
13:37:35,12 graphrag.utils.storage INFO reading table from storage: community_reports.parquet
13:37:35,37 graphrag.utils.storage INFO reading table from storage: documents.parquet
13:37:35,114 graphrag.utils.storage INFO reading table from storage: entities.parquet
13:37:35,178 graphrag.utils.storage INFO reading table from storage: relationships.parquet
13:37:35,278 graphrag.utils.storage INFO reading table from storage: text_units.parquet
13:37:35,319 graphrag.index.run.run_pipeline INFO Final # of rows loaded: 1
13:37:35,401 graphrag.utils.storage INFO reading table from storage: documents.parquet
13:37:35,501 graphrag.utils.storage INFO reading table from storage: documents.parquet
13:37:35,517 graphrag.utils.storage INFO reading table from storage: text_units.parquet
13:37:35,561 graphrag.utils.storage INFO reading table from storage: text_units.parquet
13:37:46,388 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:37:48,260 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:37:53,952 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:37:56,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:37:58,178 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:37:58,246 graphrag.utils.storage INFO reading table from storage: entities.parquet
13:37:58,260 graphrag.utils.storage INFO reading table from storage: relationships.parquet
13:37:58,321 graphrag.utils.storage INFO reading table from storage: entities.parquet
13:37:58,335 graphrag.utils.storage INFO reading table from storage: relationships.parquet
13:37:58,391 graphrag.utils.storage INFO reading table from storage: text_units.parquet
13:37:58,403 graphrag.utils.storage INFO reading table from storage: entities.parquet
13:37:58,415 graphrag.utils.storage INFO reading table from storage: relationships.parquet
13:37:58,461 graphrag.utils.storage INFO reading table from storage: relationships.parquet
13:37:58,473 graphrag.utils.storage INFO reading table from storage: entities.parquet
13:37:58,485 graphrag.utils.storage INFO reading table from storage: communities.parquet
13:37:58,506 graphrag.index.operations.summarize_communities.graph_context.context_builder INFO Number of nodes at level=0 => 15
13:38:12,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:38:14,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:38:15,397 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:38:15,447 graphrag.utils.storage INFO reading table from storage: documents.parquet
13:38:15,459 graphrag.utils.storage INFO reading table from storage: relationships.parquet
13:38:15,472 graphrag.utils.storage INFO reading table from storage: text_units.parquet
13:38:15,486 graphrag.utils.storage INFO reading table from storage: entities.parquet
13:38:15,503 graphrag.utils.storage INFO reading table from storage: community_reports.parquet
13:38:15,523 graphrag.index.workflows.generate_text_embeddings INFO Creating embeddings
13:38:15,524 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
13:38:15,638 graphrag.index.operations.embed_text.strategies.openai INFO embedding 3 inputs via 3 snippets using 1 batches. max_batch_size=16, max_tokens=8191
13:38:17,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:38:17,790 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
13:38:17,836 graphrag.index.operations.embed_text.strategies.openai INFO embedding 15 inputs via 15 snippets using 1 batches. max_batch_size=16, max_tokens=8191
13:38:18,723 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:38:19,621 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
13:38:19,682 graphrag.index.operations.embed_text.strategies.openai INFO embedding 2 inputs via 2 snippets using 1 batches. max_batch_size=16, max_tokens=8191
13:38:20,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:38:20,400 graphrag.utils.storage INFO reading table from storage: documents.parquet
13:38:20,416 graphrag.utils.storage INFO reading table from storage: documents.parquet
13:38:20,436 graphrag.utils.storage INFO reading table from storage: entities.parquet
13:38:20,448 graphrag.utils.storage INFO reading table from storage: entities.parquet
13:38:20,470 graphrag.utils.storage INFO reading table from storage: relationships.parquet
13:38:20,483 graphrag.utils.storage INFO reading table from storage: relationships.parquet
13:38:20,531 graphrag.utils.storage INFO reading table from storage: text_units.parquet
13:38:20,548 graphrag.utils.storage INFO reading table from storage: text_units.parquet
13:38:20,577 graphrag.utils.storage INFO reading table from storage: communities.parquet
13:38:20,590 graphrag.utils.storage INFO reading table from storage: communities.parquet
13:38:20,615 graphrag.utils.storage INFO reading table from storage: community_reports.parquet
13:38:20,631 graphrag.utils.storage INFO reading table from storage: community_reports.parquet
13:38:20,663 graphrag.index.workflows.generate_text_embeddings INFO Creating embeddings
13:38:20,664 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
13:38:20,699 graphrag.index.operations.embed_text.strategies.openai INFO embedding 6 inputs via 6 snippets using 1 batches. max_batch_size=16, max_tokens=8191
13:38:22,213 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:38:22,944 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
13:38:22,965 graphrag.index.operations.embed_text.strategies.openai INFO embedding 33 inputs via 33 snippets using 3 batches. max_batch_size=16, max_tokens=8191
13:38:23,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:38:24,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:38:24,225 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:38:24,915 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
13:38:24,936 graphrag.index.operations.embed_text.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
13:38:26,197 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:38:26,815 graphrag.cli.index INFO All workflows completed successfully.
14:10:24,347 graphrag.cli.index INFO Logging enabled at /app/src/graphrag/graphrag_db/logs/indexing-engine.log
14:10:24,924 openai._base_client INFO Retrying request to /chat/completions in 0.386764 seconds
14:10:26,184 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:10:27,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:10:27,260 graphrag.cli.index INFO Starting pipeline run. dry_run=False
14:10:27,261 graphrag.cli.index INFO Using default configuration: {
    "root_dir": "/app/src/graphrag/graphrag_db",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "encoding_model": "o200k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "/app/src/graphrag/graphrag_db/logs",
        "storage_account_blob_url": null
    },
    "output": {
        "type": "file",
        "base_dir": "/app/src/graphrag/graphrag_db/output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "/app/src/graphrag/graphrag_db/update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "embed_text": {
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "names": [],
        "strategy": null,
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store"
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false
    },
    "extract_graph": {
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": null,
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "summarize_descriptions": {
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "community_reports": {
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "extract_claims": {
        "enabled": false,
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40,
        "remove_ego_nodes": false,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "reduce_max_tokens": 2000,
        "reduce_temperature": 0,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 4096
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "conversation_history_max_turns": 5,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "/app/src/graphrag/graphrag_db/output/lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null
}
14:10:27,262 graphrag.storage.file_pipeline_storage INFO Creating file storage at /app/src/graphrag/graphrag_db/output
14:10:27,265 graphrag.index.input.factory INFO loading input from root_dir=input
14:10:27,265 graphrag.index.input.factory INFO using file storage for input
14:10:27,267 graphrag.storage.file_pipeline_storage INFO search /app/src/graphrag/graphrag_db/input for files matching .*\.txt$
14:10:27,284 graphrag.index.input.util INFO Found 1 InputFileType.text files, loading 1
14:10:27,285 graphrag.index.input.util INFO Total number of unfiltered text rows: 1
14:10:27,291 graphrag.index.run.run_pipeline INFO Final # of rows loaded: 1
14:10:27,323 graphrag.utils.storage INFO reading table from storage: documents.parquet
14:10:27,408 graphrag.utils.storage INFO reading table from storage: documents.parquet
14:10:27,421 graphrag.utils.storage INFO reading table from storage: text_units.parquet
14:10:27,454 graphrag.utils.storage INFO reading table from storage: text_units.parquet
14:10:28,438 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:10:28,915 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:10:30,7 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:10:30,124 graphrag.utils.storage INFO reading table from storage: entities.parquet
14:10:30,137 graphrag.utils.storage INFO reading table from storage: relationships.parquet
14:10:30,186 graphrag.utils.storage INFO reading table from storage: entities.parquet
14:10:30,205 graphrag.utils.storage INFO reading table from storage: relationships.parquet
14:10:30,269 graphrag.utils.storage INFO reading table from storage: text_units.parquet
14:10:30,283 graphrag.utils.storage INFO reading table from storage: entities.parquet
14:10:30,294 graphrag.utils.storage INFO reading table from storage: relationships.parquet
14:10:30,351 graphrag.utils.storage INFO reading table from storage: relationships.parquet
14:10:30,369 graphrag.utils.storage INFO reading table from storage: entities.parquet
14:10:30,383 graphrag.utils.storage INFO reading table from storage: communities.parquet
14:10:30,431 graphrag.index.operations.summarize_communities.graph_context.context_builder INFO Number of nodes at level=0 => 9
14:10:39,976 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:10:41,689 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:10:41,735 graphrag.utils.storage INFO reading table from storage: documents.parquet
14:10:41,747 graphrag.utils.storage INFO reading table from storage: relationships.parquet
14:10:41,758 graphrag.utils.storage INFO reading table from storage: text_units.parquet
14:10:41,771 graphrag.utils.storage INFO reading table from storage: entities.parquet
14:10:41,785 graphrag.utils.storage INFO reading table from storage: community_reports.parquet
14:10:41,801 graphrag.index.workflows.generate_text_embeddings INFO Creating embeddings
14:10:41,802 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
14:10:41,941 graphrag.index.operations.embed_text.strategies.openai INFO embedding 2 inputs via 2 snippets using 1 batches. max_batch_size=16, max_tokens=8191
14:10:42,415 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:10:42,946 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
14:10:42,983 graphrag.index.operations.embed_text.strategies.openai INFO embedding 3 inputs via 3 snippets using 1 batches. max_batch_size=16, max_tokens=8191
14:10:43,245 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
14:10:43,278 graphrag.index.operations.embed_text.strategies.openai INFO embedding 9 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
14:10:44,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:10:45,492 graphrag.cli.index INFO All workflows completed successfully.
00:08:52,639 graphrag.cli.index INFO Logging enabled at C:\Users\ASUS\Downloads\Bachelor_research_project\backend\src\graphrag\graphrag_db\logs\indexing-engine.log
00:08:58,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:01,775 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:09:01,786 graphrag.cli.index INFO Starting pipeline run. dry_run=False
00:09:01,789 graphrag.cli.index INFO Using default configuration: {
    "root_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "encoding_model": "o200k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\logs",
        "storage_account_blob_url": null
    },
    "output": {
        "type": "file",
        "base_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "embed_text": {
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "names": [],
        "strategy": null,
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store"
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false
    },
    "extract_graph": {
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": null,
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "summarize_descriptions": {
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "community_reports": {
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "extract_claims": {
        "enabled": false,
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40,
        "remove_ego_nodes": false,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "reduce_max_tokens": 2000,
        "reduce_temperature": 0,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 4096
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "conversation_history_max_turns": 5,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "C:\\Users\\ASUS\\Downloads\\Bachelor_research_project\\backend\\src\\graphrag\\graphrag_db\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null
}
00:09:01,799 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\ASUS\Downloads\Bachelor_research_project\backend\src\graphrag\graphrag_db\output
00:09:01,802 graphrag.index.input.factory INFO loading input from root_dir=input
00:09:01,802 graphrag.index.input.factory INFO using file storage for input
00:09:01,809 graphrag.storage.file_pipeline_storage INFO search C:\Users\ASUS\Downloads\Bachelor_research_project\backend\src\graphrag\graphrag_db\input for files matching .*\.txt$
00:09:02,167 graphrag.index.input.util INFO Found 14 InputFileType.text files, loading 14
00:09:02,172 graphrag.index.input.util INFO Total number of unfiltered InputFileType.text rows: 14
00:09:02,185 graphrag.index.run.run_pipeline INFO Final # of rows loaded: 14
00:09:02,297 graphrag.utils.storage INFO reading table from storage: documents.parquet
00:09:02,669 graphrag.utils.storage INFO reading table from storage: documents.parquet
00:09:02,683 graphrag.utils.storage INFO reading table from storage: text_units.parquet
00:09:02,870 graphrag.utils.storage INFO reading table from storage: text_units.parquet
00:09:08,14 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:08,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:08,661 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:08,670 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:08,855 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:08,883 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:09,450 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:09,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:09,721 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:10,899 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:11,45 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:11,596 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:12,885 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:13,642 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:13,685 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:13,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:14,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:14,399 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:15,237 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:15,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:15,276 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:15,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:15,969 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:16,307 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:16,395 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:16,539 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:16,679 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:17,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:17,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:18,359 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:19,709 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:19,915 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:20,336 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:20,509 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:20,821 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:21,338 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:21,347 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:21,670 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:21,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:21,715 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:21,803 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:21,809 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:22,453 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:22,793 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:23,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:23,663 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:23,989 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:24,593 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:24,699 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:24,989 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:25,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:25,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:25,883 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:25,916 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:26,189 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:26,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:26,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:27,375 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:27,726 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:28,259 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:28,595 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:28,803 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:29,29 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:29,106 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:29,113 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:29,139 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:29,475 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:30,475 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:30,645 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:30,681 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:30,684 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:30,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:30,889 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:31,189 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:31,279 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:31,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:31,545 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:31,809 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:31,815 openai._base_client INFO Retrying request to /chat/completions in 0.435000 seconds
00:09:32,91 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:32,330 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:32,539 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:32,579 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:32,579 openai._base_client INFO Retrying request to /chat/completions in 0.085000 seconds
00:09:33,333 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:33,491 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:33,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:33,934 openai._base_client INFO Retrying request to /chat/completions in 0.503000 seconds
00:09:34,579 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:34,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:34,833 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:35,15 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:35,15 openai._base_client INFO Retrying request to /chat/completions in 0.590000 seconds
00:09:35,43 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:35,45 openai._base_client INFO Retrying request to /chat/completions in 0.524000 seconds
00:09:35,145 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:35,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:35,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:35,695 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:35,728 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:35,903 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:35,904 openai._base_client INFO Retrying request to /chat/completions in 0.396000 seconds
00:09:35,919 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:35,919 openai._base_client INFO Retrying request to /chat/completions in 0.391000 seconds
00:09:35,919 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:35,922 openai._base_client INFO Retrying request to /chat/completions in 0.217000 seconds
00:09:36,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:36,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:36,83 openai._base_client INFO Retrying request to /chat/completions in 0.443000 seconds
00:09:36,279 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:36,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:36,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:36,435 openai._base_client INFO Retrying request to /chat/completions in 0.249000 seconds
00:09:36,675 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:36,680 openai._base_client INFO Retrying request to /chat/completions in 0.231000 seconds
00:09:36,680 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:36,683 openai._base_client INFO Retrying request to /chat/completions in 0.214000 seconds
00:09:36,870 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:36,870 openai._base_client INFO Retrying request to /chat/completions in 0.005000 seconds
00:09:37,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:37,190 openai._base_client INFO Retrying request to /chat/completions in 0.323000 seconds
00:09:37,205 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:37,207 openai._base_client INFO Retrying request to /chat/completions in 0.281000 seconds
00:09:37,224 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:37,225 openai._base_client INFO Retrying request to /chat/completions in 0.276000 seconds
00:09:37,368 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:37,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:37,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:37,833 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:37,837 openai._base_client INFO Retrying request to /chat/completions in 0.031000 seconds
00:09:37,847 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:37,848 openai._base_client INFO Retrying request to /chat/completions in 0.035000 seconds
00:09:37,856 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:37,859 openai._base_client INFO Retrying request to /chat/completions in 0.040000 seconds
00:09:38,201 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:38,203 openai._base_client INFO Retrying request to /chat/completions in 0.513000 seconds
00:09:38,269 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:38,269 openai._base_client INFO Retrying request to /chat/completions in 0.476000 seconds
00:09:38,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:38,793 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:39,31 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:39,33 openai._base_client INFO Retrying request to /chat/completions in 0.063000 seconds
00:09:39,404 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:39,406 openai._base_client INFO Retrying request to /chat/completions in 0.492000 seconds
00:09:39,910 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:40,64 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:40,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:09:40,223 openai._base_client INFO Retrying request to /chat/completions in 0.297000 seconds
00:09:40,273 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:41,929 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:42,325 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:48,189 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:48,443 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:50,102 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:52,355 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:54,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,29 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,38 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,188 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,355 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,362 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,442 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,460 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,670 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,763 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:57,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,275 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,345 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,715 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,825 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,849 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,903 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,974 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:58,979 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,288 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,375 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,438 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,455 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,895 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,966 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:09:59,989 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,25 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,70 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,213 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,304 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,412 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,446 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,526 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,539 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,755 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,795 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,835 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:00,875 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,153 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,425 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,720 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,774 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,795 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:01,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:02,42 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:02,108 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:02,115 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:02,313 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:03,188 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:04,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:04,715 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:04,772 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:04,775 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:04,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:04,803 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:04,889 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:04,929 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:04,943 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,126 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,171 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,181 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,214 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,261 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,392 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,527 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,647 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:05,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,15 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,18 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,55 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,309 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,428 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,604 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:06,669 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:08,609 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:09,103 graphrag.utils.storage INFO reading table from storage: entities.parquet
00:10:09,155 graphrag.utils.storage INFO reading table from storage: relationships.parquet
00:10:09,499 graphrag.utils.storage INFO reading table from storage: entities.parquet
00:10:09,551 graphrag.utils.storage INFO reading table from storage: relationships.parquet
00:10:10,95 graphrag.utils.storage INFO reading table from storage: text_units.parquet
00:10:10,119 graphrag.utils.storage INFO reading table from storage: entities.parquet
00:10:10,138 graphrag.utils.storage INFO reading table from storage: relationships.parquet
00:10:10,551 graphrag.utils.storage INFO reading table from storage: relationships.parquet
00:10:10,567 graphrag.utils.storage INFO reading table from storage: entities.parquet
00:10:10,593 graphrag.utils.storage INFO reading table from storage: communities.parquet
00:10:10,782 graphrag.index.operations.summarize_communities.graph_context.context_builder INFO Number of nodes at level=2 => 22
00:10:11,42 graphrag.index.operations.summarize_communities.graph_context.context_builder INFO Number of nodes at level=1 => 299
00:10:13,54 graphrag.index.operations.summarize_communities.graph_context.context_builder INFO Number of nodes at level=0 => 308
00:10:24,835 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:27,54 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:27,714 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:27,925 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:30,555 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:38,574 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:40,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:40,289 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:40,375 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:40,428 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:40,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:40,538 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:40,593 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:40,689 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:40,913 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:41,294 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:41,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:41,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:42,612 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:42,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:42,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:42,912 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:43,42 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:44,115 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:44,152 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:44,565 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:44,849 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:45,948 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:46,889 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:48,674 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:48,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:49,657 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:51,132 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:51,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:51,462 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:51,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:52,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:52,388 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:52,444 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:52,517 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:52,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:52,706 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:52,943 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:53,415 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:53,594 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:53,603 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:53,845 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:53,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:53,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:54,986 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:57,386 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:10:58,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
00:10:58,414 openai._base_client INFO Retrying request to /chat/completions in 0.480000 seconds
00:11:07,514 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:08,330 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:08,558 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:08,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:09,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:09,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:09,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:09,778 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:10,102 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:10,132 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:11,160 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:11,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:12,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:13,259 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:15,765 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
00:11:16,101 graphrag.utils.storage INFO reading table from storage: documents.parquet
00:11:16,142 graphrag.utils.storage INFO reading table from storage: relationships.parquet
00:11:16,181 graphrag.utils.storage INFO reading table from storage: text_units.parquet
00:11:16,228 graphrag.utils.storage INFO reading table from storage: entities.parquet
00:11:16,250 graphrag.utils.storage INFO reading table from storage: community_reports.parquet
00:11:16,304 graphrag.index.workflows.generate_text_embeddings INFO Creating embeddings
00:11:16,308 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
00:11:16,618 graphrag.index.operations.embed_text.strategies.openai INFO embedding 66 inputs via 66 snippets using 6 batches. max_batch_size=16, max_tokens=8191
00:11:17,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:17,672 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:17,741 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:17,802 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:18,120 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:18,725 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:19,192 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
00:11:19,313 graphrag.index.operations.embed_text.strategies.openai INFO embedding 342 inputs via 342 snippets using 22 batches. max_batch_size=16, max_tokens=8191
00:11:19,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:19,905 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:19,914 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:19,948 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,145 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,165 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,192 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,195 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,212 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,218 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,221 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,228 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,244 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,364 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,453 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:20,646 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:21,99 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:21,574 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:22,382 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
00:11:22,577 graphrag.index.operations.embed_text.strategies.openai INFO embedding 57 inputs via 57 snippets using 8 batches. max_batch_size=16, max_tokens=8191
00:11:23,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:23,412 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:23,434 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:23,438 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:23,743 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:23,746 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:23,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:23,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
00:11:24,876 graphrag.cli.index INFO All workflows completed successfully.
